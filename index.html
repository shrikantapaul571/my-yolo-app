<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>YOLOv8 Live Detection</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
    <style>
        body {
            margin: 0;
            padding: 0;
            background-color: #1a1a1a;
            color: white;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            font-family: Arial, sans-serif;
        }

        .container {
            position: relative;
            width: 640px;
            height: 640px;
            border: 2px solid #00ff00;
            box-shadow: 0 0 20px rgba(0, 255, 0, 0.2);
        }

        /* Make video and canvas overlap perfectly */
        video {
            position: absolute;
            top: 0;
            left: 0;
            width: 640px;
            height: 640px;
            object-fit: cover;
            z-index: 1;
        }

        canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 640px;
            height: 640px;
            z-index: 2;
        }

        #status {
            margin-top: 10px;
            font-size: 18px;
            color: #00ff00;
        }
    </style>
</head>
<body>

    <div class="container">
        <video id="webcam" autoplay playsinline muted></video>
        <canvas id="canvas" width="640" height="640"></canvas>
    </div>
    <div id="status">Loading Model...</div>

    <script>
        // 1. CONFIGURATION
        const MODEL_URL = './best11n_web_model/model.json'; 
        const LABELS = ['Oil', 'jam', 'pasta', 'rice', 'soda', 'tomato_sauce'];
        const CONFIDENCE_THRESHOLD = 0.50; // 50% confidence required
        const IOU_THRESHOLD = 0.45;        // Overlap threshold for NMS

        const video = document.getElementById('webcam');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const statusText = document.getElementById('status');
        let model;

        // 2. MAIN SETUP
        async function setup() {
            try {
                // Load Camera
                const stream = await navigator.mediaDevices.getUserMedia({
                    video: { facingMode: 'environment', width: 640, height: 640 },
                    audio: false
                });
                video.srcObject = stream;

                // Load Model
                statusText.innerText = "Loading Model...";
                model = await tf.loadGraphModel(MODEL_URL);
                statusText.innerText = "Model Loaded! Detecting...";
                
                // Wait for video to be ready
                video.onloadedmetadata = () => {
                    detectFrame();
                };
            } catch (err) {
                console.error(err);
                statusText.innerText = "Error: " + err.message;
            }
        }

        // 3. DETECTION LOOP
        async function detectFrame() {
            // Ensure video is playing and has data
            if (video.readyState !== 4) {
                requestAnimationFrame(detectFrame);
                return;
            }

            // A. Pre-process Image
            // YOLOv8 expects [1, 640, 640, 3] float32 tensor normalized 0-1
            const input = tf.tidy(() => {
                return tf.image.resizeBilinear(tf.browser.fromPixels(video), [640, 640])
                    .div(255.0)
                    .expandDims(0);
            });

            // B. Run Inference
            const res = await model.executeAsync(input); // Returns [1, 84, 8400]
            
            // C. Post-Process (Decode YOLO Output)
            const [boxes, scores, classes] = processOutput(res);

            // D. Draw Results
            drawBoxes(boxes, scores, classes);

            // Cleanup memory
            tf.dispose([res, input]);

            // Loop
            requestAnimationFrame(detectFrame);
        }

        // 4. DECODING YOLO OUTPUT
        function processOutput(res) {
            const transRes = res.transpose([0, 2, 1]); // [1, 8400, 84]
            const data = transRes.dataSync();
            
            const boxes = [];
            const scores = [];
            const classIndices = [];

            // Iterate through all 8400 potential detections
            for (let i = 0; i < 8400; i++) {
                const row = i * 84; // 84 = 4 box coords + 80 classes (even if you have 6, model output is fixed usually)
                
                // Find max class score
                let maxScore = 0;
                let maxClass = -1;
                
                // We only check your 6 classes (indices 4 to 9)
                for (let j = 0; j < LABELS.length; j++) {
                    const score = data[row + 4 + j];
                    if (score > maxScore) {
                        maxScore = score;
                        maxClass = j;
                    }
                }

                if (maxScore > CONFIDENCE_THRESHOLD) {
                    const x = data[row];
                    const y = data[row + 1];
                    const w = data[row + 2];
                    const h = data[row + 3];

                    // Convert [x, y, w, h] to [x1, y1, x2, y2]
                    boxes.push([
                        (x - w / 2) * 640, 
                        (y - h / 2) * 640, 
                        (x + w / 2) * 640, 
                        (y + h / 2) * 640 
                    ]);
                    scores.push(maxScore);
                    classIndices.push(maxClass);
                }
            }

            // Non-Max Suppression (Remove overlapping boxes)
            if (boxes.length > 0) {
                const nmsIndices = tf.image.nonMaxSuppression(
                    tf.tensor2d(boxes), 
                    tf.tensor1d(scores), 
                    100, // Max output boxes
                    IOU_THRESHOLD, 
                    CONFIDENCE_THRESHOLD
                ).dataSync();
                
                transRes.dispose();
                return [
                    nmsIndices.map(i => boxes[i]),
                    nmsIndices.map(i => scores[i]),
                    nmsIndices.map(i => classIndices[i])
                ];
            }

            transRes.dispose();
            return [[], [], []];
        }

        // 5. DRAWING
        function drawBoxes(boxes, scores, classes) {
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            ctx.font = "18px Arial";
            ctx.lineWidth = 3;

            boxes.forEach((box, i) => {
                const [x1, y1, x2, y2] = box;
                const label = LABELS[classes[i]];
                const score = (scores[i] * 100).toFixed(1);
                const color = "#00FF00";

                // Draw Box
                ctx.strokeStyle = color;
                ctx.strokeRect(x1, y1, x2 - x1, y2 - y1);

                // Draw Label Background
                const text = `${label} ${score}%`;
                const textWidth = ctx.measureText(text).width;
                ctx.fillStyle = color;
                ctx.fillRect(x1, y1 - 25, textWidth + 10, 25);

                // Draw Text
                ctx.fillStyle = "black";
                ctx.fillText(text, x1 + 5, y1 - 5);
            });
        }

        // Start App
        setup();
    </script>
</body>
</html>
